{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784)/255.0\n",
    "x_test = x_test.reshape(10000,784)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = list(range(10))\n",
    "x_train_1000, y_train_1000 = [], []\n",
    "x_test_1000, y_test_1000 = [], []\n",
    "\n",
    "train_1000 = None\n",
    "test_1000 = None\n",
    "\n",
    "d1 = None\n",
    "d2 = None\n",
    "\n",
    "Train1 = np.append(x_train, y_train.reshape(len(y_train), 1), axis = 1)\n",
    "Test1 = np.append(x_test, y_test.reshape(len(y_test), 1), axis = 1)\n",
    "\n",
    "for d in digits:\n",
    "    Sample_train = Train1[y_train == d]\n",
    "    Sample_test = Test1[y_test == d]\n",
    "    if (d1 == None):\n",
    "        train_1000 = Sample_train[np.random.randint(Sample_train.shape[0], size=100), :]\n",
    "        d1 = 5\n",
    "    else:\n",
    "        train_1000 = np.append(train_1000, Sample_train[np.random.randint(Sample_train.shape[0], size=100), :], axis = 0)\n",
    "    \n",
    "    if (d2 == None):\n",
    "        test_1000 = Sample_test[np.random.randint(Sample_test.shape[0], size=100), :]\n",
    "        d2 = 5\n",
    "    else:\n",
    "        test_1000 = np.append(test_1000, Sample_test[np.random.randint(Sample_test.shape[0], size=100), :],axis = 0)\n",
    "\n",
    "np.random.shuffle(train_1000)\n",
    "np.random.shuffle(test_1000)\n",
    "\n",
    "x_train_1000 = train_1000[:, 0:-1]\n",
    "x_test_1000 = test_1000[:,0:-1]\n",
    "y_train_1000 = train_1000[:, -1]\n",
    "y_test_1000 = test_1000[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax')))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "NN = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(NN, x_test_1000, y_test_1000):\n",
    "    y_pred = NN.predict(x_test_1000)\n",
    "    test_acc = metrics.accuracy_score(y_true=y_test_1000, y_pred=y_pred)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Network,x_train_1000,y_train_1000,x_test_1000,y_test_1000,epochs,no_of_L):\n",
    "    Network_Acc = []\n",
    "    Network_VAcc = []\n",
    "    Network_Train_Loss = []\n",
    "    Network_Train_VLoss = []\n",
    "    Network_AvgWChange = []\n",
    "    Prev_Layr_W = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        Prev_Layr_W = []\n",
    "\n",
    "        for I in range(no_of_L):\n",
    "            Layr_W =  np.array(Network.get_layer(index = I).get_weights()[0])\n",
    "            Prev_Layr_W.append(Layr_W)\n",
    "\n",
    "        History_1 = Network.fit(x = x_train_1000, y= y_train_1000, batch_size= 10, epochs=1, validation_data=(x_test_1000, y_test_1000))\n",
    "        Network_Acc.append(History_1.history['accuracy'])\n",
    "        Network_VAcc.append(History_1.history['val_accuracy'])\n",
    "        Network_Train_Loss.append(History_1.history['loss'])\n",
    "        Network_Train_VLoss.append(History_1.history['val_loss'])\n",
    "\n",
    "        for I in range(no_of_L):\n",
    "            Layr_W =  np.array(Network.get_layer(index = I).get_weights()[0])\n",
    "            Avg_Change_in_L = np.average(np.absolute((Prev_Layr_W[I] - Layr_W)/Layr_W))\n",
    "            print(Avg_Change_in_L, end = ' ')\n",
    "            if e == 0:\n",
    "                Network_AvgWChange.append([Avg_Change_in_L])\n",
    "            else:\n",
    "                Network_AvgWChange[I].append(Avg_Change_in_L)\n",
    "        \n",
    "        print()\n",
    "\n",
    "    return [Network_Acc, Network_VAcc, Network_Train_Loss, Network_Train_VLoss, Network_AvgWChange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 642us/sample - loss: 1.6547 - accuracy: 0.4330 - val_loss: 1.2595 - val_accuracy: 0.5890\n",
      "0.92495036 2.8521917 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 261us/sample - loss: 0.9535 - accuracy: 0.6940 - val_loss: 1.0598 - val_accuracy: 0.6360\n",
      "1.3974419 1.1248822 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 0.8294 - accuracy: 0.7290 - val_loss: 0.9870 - val_accuracy: 0.7540\n",
      "1.8303914 0.7419769 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 0.7349 - accuracy: 0.7830 - val_loss: 0.9489 - val_accuracy: 0.7290\n",
      "1.0295725 0.6423755 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 271us/sample - loss: 0.6182 - accuracy: 0.8140 - val_loss: 0.6869 - val_accuracy: 0.8160\n",
      "0.98706704 1.3307427 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 0.7305 - accuracy: 0.8160 - val_loss: 1.0041 - val_accuracy: 0.7920\n",
      "1.7340527 0.68440604 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 0.6016 - accuracy: 0.8430 - val_loss: 0.9091 - val_accuracy: 0.7980\n",
      "0.8095443 0.50258106 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 0.5275 - accuracy: 0.8650 - val_loss: 0.9121 - val_accuracy: 0.7990\n",
      "0.8122329 0.35731527 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 326us/sample - loss: 0.4415 - accuracy: 0.8800 - val_loss: 0.8491 - val_accuracy: 0.8130\n",
      "0.8720464 0.5123802 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 355us/sample - loss: 0.4835 - accuracy: 0.8850 - val_loss: 0.8645 - val_accuracy: 0.8210\n",
      "0.85606563 0.5355691 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.4705 - accuracy: 0.8830 - val_loss: 1.1507 - val_accuracy: 0.8070\n",
      "1.421565 0.6589798 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 275us/sample - loss: 0.3774 - accuracy: 0.9010 - val_loss: 1.0779 - val_accuracy: 0.8020\n",
      "0.6396503 0.8225812 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 364us/sample - loss: 0.3915 - accuracy: 0.8960 - val_loss: 0.9865 - val_accuracy: 0.8310\n",
      "0.9000941 0.32696044 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 429us/sample - loss: 0.3670 - accuracy: 0.9140 - val_loss: 1.0287 - val_accuracy: 0.8180\n",
      "0.7040599 0.3408759 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 0.2992 - accuracy: 0.9270 - val_loss: 1.0934 - val_accuracy: 0.8170\n",
      "0.8206906 0.26911107 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 0.3224 - accuracy: 0.9170 - val_loss: 1.1563 - val_accuracy: 0.8150\n",
      "0.6132142 0.27822876 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.2446 - accuracy: 0.9340 - val_loss: 1.1770 - val_accuracy: 0.8230\n",
      "0.6996318 0.36642757 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 0.2306 - accuracy: 0.9450 - val_loss: 1.2230 - val_accuracy: 0.8220\n",
      "0.53728443 0.58056146 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 0.2646 - accuracy: 0.9420 - val_loss: 1.1054 - val_accuracy: 0.8360\n",
      "0.54944867 0.2883728 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 0.1704 - accuracy: 0.9580 - val_loss: 1.3353 - val_accuracy: 0.8180\n",
      "1.2422975 0.43237326 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 371us/sample - loss: 0.2371 - accuracy: 0.9540 - val_loss: 1.2626 - val_accuracy: 0.8360\n",
      "0.99950475 0.21775357 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 0.1859 - accuracy: 0.9570 - val_loss: 1.4520 - val_accuracy: 0.8110\n",
      "0.46218196 0.21808685 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 325us/sample - loss: 0.2154 - accuracy: 0.9430 - val_loss: 1.4616 - val_accuracy: 0.8240\n",
      "0.5617678 0.219164 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 0.1361 - accuracy: 0.9570 - val_loss: 1.3968 - val_accuracy: 0.8200\n",
      "0.9817142 0.26474136 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 0.1744 - accuracy: 0.9500 - val_loss: 1.3111 - val_accuracy: 0.8260\n",
      "0.5878678 0.5761042 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 0.1834 - accuracy: 0.9570 - val_loss: 1.4901 - val_accuracy: 0.8160\n",
      "0.52507865 0.22016662 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.1479 - accuracy: 0.9660 - val_loss: 1.6568 - val_accuracy: 0.8150\n",
      "0.35696283 0.22970718 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.1033 - accuracy: 0.9740 - val_loss: 1.5168 - val_accuracy: 0.8230\n",
      "0.35697004 0.2530313 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 279us/sample - loss: 0.0908 - accuracy: 0.9760 - val_loss: 1.4899 - val_accuracy: 0.8300\n",
      "0.37446338 0.33567697 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 0.0960 - accuracy: 0.9710 - val_loss: 1.7202 - val_accuracy: 0.8260\n",
      "0.47852018 0.11233783 \n"
     ]
    }
   ],
   "source": [
    "NN = network()\n",
    "Results = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30, no_of_L = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax')))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 809us/sample - loss: 1.9234 - accuracy: 0.3140 - val_loss: 1.3614 - val_accuracy: 0.4820\n",
      "0.8483626 1.7997879 2.7825506 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 1.2075 - accuracy: 0.5860 - val_loss: 1.1084 - val_accuracy: 0.6310\n",
      "1.7719063 1.5981678 1.3454212 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 383us/sample - loss: 0.9512 - accuracy: 0.6750 - val_loss: 1.1716 - val_accuracy: 0.6240\n",
      "1.2441466 1.2516621 1.8475051 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 0.9171 - accuracy: 0.7000 - val_loss: 1.1089 - val_accuracy: 0.6790\n",
      "1.1531491 1.0382727 0.7319347 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 0.8257 - accuracy: 0.7640 - val_loss: 0.9834 - val_accuracy: 0.7390\n",
      "1.0238717 6.010477 1.4156393 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 0.7727 - accuracy: 0.7840 - val_loss: 0.8268 - val_accuracy: 0.7330\n",
      "0.8574746 7.6252685 0.47147033 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 0.7311 - accuracy: 0.7880 - val_loss: 1.1547 - val_accuracy: 0.7130\n",
      "0.8231844 1.0184519 0.7822833 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 361us/sample - loss: 0.7376 - accuracy: 0.8000 - val_loss: 0.8719 - val_accuracy: 0.7470\n",
      "0.832302 1.5026174 0.43954742 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 0.6906 - accuracy: 0.8200 - val_loss: 1.2409 - val_accuracy: 0.7060\n",
      "0.73968846 1.0832739 0.5370757 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 363us/sample - loss: 0.6067 - accuracy: 0.8230 - val_loss: 1.0289 - val_accuracy: 0.7720\n",
      "0.9394518 1.7038355 0.44324848 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 0.5951 - accuracy: 0.8550 - val_loss: 1.1683 - val_accuracy: 0.7540\n",
      "1.0829456 6.23788 0.23508637 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 352us/sample - loss: 0.6824 - accuracy: 0.8370 - val_loss: 1.0590 - val_accuracy: 0.7740\n",
      "1.1867756 0.6512601 2.427966 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 0.7226 - accuracy: 0.8260 - val_loss: 1.1408 - val_accuracy: 0.7390\n",
      "0.656627 1.3106982 0.402868 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 365us/sample - loss: 0.6532 - accuracy: 0.8210 - val_loss: 1.2594 - val_accuracy: 0.7800\n",
      "0.744793 0.7472496 0.532677 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 382us/sample - loss: 0.6844 - accuracy: 0.8230 - val_loss: 1.1997 - val_accuracy: 0.7610\n",
      "0.74878156 0.616554 0.2797497 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 0.6936 - accuracy: 0.8260 - val_loss: 1.0693 - val_accuracy: 0.8040\n",
      "0.66577655 0.7290679 0.19150539 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 278us/sample - loss: 0.5462 - accuracy: 0.8670 - val_loss: 0.9338 - val_accuracy: 0.8050\n",
      "0.5928904 1.0176823 0.6827717 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 280us/sample - loss: 0.5512 - accuracy: 0.8680 - val_loss: 1.1953 - val_accuracy: 0.7950\n",
      "0.51573455 1.1314253 0.14390321 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 332us/sample - loss: 0.6027 - accuracy: 0.8680 - val_loss: 1.3084 - val_accuracy: 0.7760\n",
      "0.51923037 0.82947725 0.21475147 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 0.4380 - accuracy: 0.8940 - val_loss: 1.0709 - val_accuracy: 0.7960\n",
      "0.5672512 0.73631305 0.13504006 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 0.4265 - accuracy: 0.8940 - val_loss: 1.2262 - val_accuracy: 0.7770\n",
      "0.64877474 0.5148829 0.35824305 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 0.6165 - accuracy: 0.8770 - val_loss: 1.0759 - val_accuracy: 0.8190\n",
      "0.52377 0.4902568 0.13217944 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 279us/sample - loss: 0.4633 - accuracy: 0.9040 - val_loss: 1.3532 - val_accuracy: 0.8160\n",
      "0.6292844 0.6196229 0.18267645 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 273us/sample - loss: 0.5328 - accuracy: 0.8990 - val_loss: 1.4915 - val_accuracy: 0.7970\n",
      "0.43892482 0.9453627 0.1154856 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 0.5449 - accuracy: 0.8940 - val_loss: 1.2100 - val_accuracy: 0.8170\n",
      "0.47953105 0.40591636 0.1286864 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 263us/sample - loss: 0.3730 - accuracy: 0.9230 - val_loss: 1.7274 - val_accuracy: 0.8150\n",
      "0.5722472 0.83473223 0.14090058 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 335us/sample - loss: 0.4118 - accuracy: 0.9140 - val_loss: 1.7907 - val_accuracy: 0.8220\n",
      "0.747091 0.6077573 0.12372485 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 364us/sample - loss: 0.4607 - accuracy: 0.9220 - val_loss: 1.5674 - val_accuracy: 0.8060\n",
      "0.38682073 12.899444 0.08978207 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 408us/sample - loss: 0.3880 - accuracy: 0.9080 - val_loss: 1.6337 - val_accuracy: 0.8140\n",
      "0.33698443 0.5215956 0.094542556 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 396us/sample - loss: 0.3225 - accuracy: 0.9320 - val_loss: 1.6573 - val_accuracy: 0.8250\n",
      "0.32351804 0.379272 0.14378169 \n"
     ]
    }
   ],
   "source": [
    "NN = network()\n",
    "Results_2 = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30, no_of_L = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid')))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax')))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 838us/sample - loss: 2.3152 - accuracy: 0.1250 - val_loss: 1.8807 - val_accuracy: 0.1880\n",
      "0.85892045 2.0476885 2.4520583 14.28678 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 1.8848 - accuracy: 0.2540 - val_loss: 1.7751 - val_accuracy: 0.2660\n",
      "2.395362 2.0261607 2.3541622 3.0036368 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 404us/sample - loss: 1.5422 - accuracy: 0.4390 - val_loss: 1.6744 - val_accuracy: 0.4940\n",
      "3.2468271 1.5801268 2.4609656 0.8213059 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 1.3589 - accuracy: 0.5160 - val_loss: 1.2496 - val_accuracy: 0.5880\n",
      "1.1050581 1.0321934 0.9316332 1.5097795 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 1.2517 - accuracy: 0.5730 - val_loss: 1.2449 - val_accuracy: 0.5910\n",
      "1.2811297 5.3507824 1.4951496 0.7497661 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 337us/sample - loss: 1.1711 - accuracy: 0.6030 - val_loss: 1.2212 - val_accuracy: 0.6000\n",
      "1.6959757 2.0688543 0.94762677 0.61435956 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 1.0536 - accuracy: 0.6770 - val_loss: 1.2560 - val_accuracy: 0.5850\n",
      "2.1250408 0.6269272 1.1131366 0.43735576 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.9971 - accuracy: 0.6950 - val_loss: 1.1258 - val_accuracy: 0.6670\n",
      "1.1411139 0.62518036 2.3240185 0.24500427 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 1.0993 - accuracy: 0.6550 - val_loss: 1.0939 - val_accuracy: 0.7160\n",
      "0.87846 0.67179793 1.2889829 9.716937 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 381us/sample - loss: 1.0209 - accuracy: 0.7140 - val_loss: 1.2483 - val_accuracy: 0.6780\n",
      "0.7578784 1.0119487 0.63446355 0.67074245 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 373us/sample - loss: 0.9496 - accuracy: 0.7340 - val_loss: 1.2113 - val_accuracy: 0.6300\n",
      "0.68101513 0.83107615 0.63413465 0.38631424 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 370us/sample - loss: 0.9672 - accuracy: 0.7470 - val_loss: 1.3557 - val_accuracy: 0.7180\n",
      "0.8326622 0.6992528 0.76424414 0.30288538 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 0.9368 - accuracy: 0.7660 - val_loss: 1.1104 - val_accuracy: 0.7180\n",
      "0.75445217 1.1603149 0.94851303 0.19570573 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 282us/sample - loss: 0.9760 - accuracy: 0.7390 - val_loss: 1.1823 - val_accuracy: 0.6590\n",
      "1.1202098 2.2642028 0.8248294 0.26468995 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 0.9711 - accuracy: 0.7370 - val_loss: 1.4603 - val_accuracy: 0.6760\n",
      "0.5876035 0.6954604 0.6072103 0.79301745 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 1.0233 - accuracy: 0.7350 - val_loss: 1.3463 - val_accuracy: 0.7070\n",
      "0.8827076 0.8418715 0.58110285 0.7765507 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 0.9495 - accuracy: 0.7690 - val_loss: 1.7952 - val_accuracy: 0.7310\n",
      "5.290303 0.76606 2.996362 0.39380258 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 0.7897 - accuracy: 0.7920 - val_loss: 1.3612 - val_accuracy: 0.7090\n",
      "1.0281096 0.63882905 1.0647962 0.28552794 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 404us/sample - loss: 0.8853 - accuracy: 0.7910 - val_loss: 1.4686 - val_accuracy: 0.6490\n",
      "0.51935124 8.079543 2.443663 0.1691745 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 374us/sample - loss: 0.9358 - accuracy: 0.7910 - val_loss: 2.0911 - val_accuracy: 0.6940\n",
      "0.5859161 0.7676511 0.6958397 0.7269192 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 337us/sample - loss: 0.9312 - accuracy: 0.7750 - val_loss: 1.2831 - val_accuracy: 0.6730\n",
      "0.6469712 1.8365617 0.62423736 0.088612325 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 395us/sample - loss: 0.7948 - accuracy: 0.8060 - val_loss: 1.2156 - val_accuracy: 0.7620\n",
      "0.54146004 1.1622348 0.5783875 0.09973216 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 0.9308 - accuracy: 0.7850 - val_loss: 1.1797 - val_accuracy: 0.7560\n",
      "0.5251762 1.4196764 1.4468095 0.3031825 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 0.7441 - accuracy: 0.8300 - val_loss: 1.2928 - val_accuracy: 0.7660\n",
      "0.5973956 0.92099977 0.5738102 0.17370407 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 0.7846 - accuracy: 0.8200 - val_loss: 1.6770 - val_accuracy: 0.7230\n",
      "0.6660589 0.64945525 0.9206073 0.1593413 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 354us/sample - loss: 0.7819 - accuracy: 0.8190 - val_loss: 1.3414 - val_accuracy: 0.7670\n",
      "0.5234596 0.5274255 0.5746228 0.15644512 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 346us/sample - loss: 0.7949 - accuracy: 0.8160 - val_loss: 1.3255 - val_accuracy: 0.7660\n",
      "0.5955254 0.80674124 1.1954621 0.24883296 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 0.6846 - accuracy: 0.8430 - val_loss: 1.5201 - val_accuracy: 0.7760\n",
      "0.5669436 0.8453943 0.8311328 0.115867466 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 0.7653 - accuracy: 0.8650 - val_loss: 1.7221 - val_accuracy: 0.7620\n",
      "1.2559829 0.56603 2.4362416 0.21825607 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 0.8625 - accuracy: 0.8240 - val_loss: 1.3948 - val_accuracy: 0.7490\n",
      "0.49887812 0.546342 1.2833794 0.07868706 \n"
     ]
    }
   ],
   "source": [
    "NN = Network()\n",
    "Results_3 = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30,no_of_L = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid', kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax')))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 342.9950 - accuracy: 0.1010 - val_loss: 293.9935 - val_accuracy: 0.0950\n",
      "0.9050612 1.6069549 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 297.4390 - accuracy: 0.1060 - val_loss: 295.9998 - val_accuracy: 0.1000\n",
      "0.02687546 0.51320475 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 297.4883 - accuracy: 0.1010 - val_loss: 296.8344 - val_accuracy: 0.0920\n",
      "0.017220195 0.29167563 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 297.5827 - accuracy: 0.0920 - val_loss: 298.0369 - val_accuracy: 0.1060\n",
      "0.021188807 0.2662758 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 278us/sample - loss: 297.5287 - accuracy: 0.1050 - val_loss: 298.1065 - val_accuracy: 0.1000\n",
      "0.017837742 0.19442119 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 288us/sample - loss: 297.4736 - accuracy: 0.1090 - val_loss: 296.3722 - val_accuracy: 0.1000\n",
      "0.030788414 0.14764868 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 293us/sample - loss: 297.4746 - accuracy: 0.1110 - val_loss: 297.0835 - val_accuracy: 0.1000\n",
      "0.026854873 0.1347241 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 282us/sample - loss: 297.4915 - accuracy: 0.1080 - val_loss: 297.3339 - val_accuracy: 0.0700\n",
      "0.023931641 0.1467814 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 297.4034 - accuracy: 0.0980 - val_loss: 296.7997 - val_accuracy: 0.1400\n",
      "0.030396063 0.09966468 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 410us/sample - loss: 297.3513 - accuracy: 0.1120 - val_loss: 297.6777 - val_accuracy: 0.1060\n",
      "0.033581242 0.10677624 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 272us/sample - loss: 297.5305 - accuracy: 0.1010 - val_loss: 296.2797 - val_accuracy: 0.1000\n",
      "0.08362535 0.10503431 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 297.4067 - accuracy: 0.1010 - val_loss: 297.7477 - val_accuracy: 0.1000\n",
      "0.032030895 0.06701546 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 386us/sample - loss: 297.4105 - accuracy: 0.1060 - val_loss: 294.8369 - val_accuracy: 0.1000\n",
      "0.039298546 0.07027394 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 297.4316 - accuracy: 0.0960 - val_loss: 297.2003 - val_accuracy: 0.1000\n",
      "0.043881606 0.08194283 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 297.4597 - accuracy: 0.0970 - val_loss: 296.0387 - val_accuracy: 0.1490\n",
      "0.032670055 0.06375046 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 297.3137 - accuracy: 0.1080 - val_loss: 295.3714 - val_accuracy: 0.1010\n",
      "0.03912367 0.056128252 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 297.2878 - accuracy: 0.0910 - val_loss: 298.1879 - val_accuracy: 0.1270\n",
      "0.04548995 0.06299372 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 297.3237 - accuracy: 0.1040 - val_loss: 297.6558 - val_accuracy: 0.1290\n",
      "0.05845451 0.051155385 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 297.3008 - accuracy: 0.1020 - val_loss: 299.1006 - val_accuracy: 0.0300\n",
      "0.07865091 0.0495145 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 349us/sample - loss: 297.2254 - accuracy: 0.1050 - val_loss: 298.0186 - val_accuracy: 0.0960\n",
      "0.052796807 0.05073991 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 346us/sample - loss: 297.3037 - accuracy: 0.1030 - val_loss: 294.4296 - val_accuracy: 0.1060\n",
      "0.078136705 0.055127945 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 391us/sample - loss: 297.2746 - accuracy: 0.0990 - val_loss: 295.1912 - val_accuracy: 0.0480\n",
      "0.1300468 0.03982042 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 328us/sample - loss: 297.2303 - accuracy: 0.0960 - val_loss: 299.0804 - val_accuracy: 0.1000\n",
      "0.3044358 0.044660974 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 293us/sample - loss: 297.3402 - accuracy: 0.0930 - val_loss: 296.0444 - val_accuracy: 0.1410\n",
      "0.054075338 0.040696874 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 388us/sample - loss: 297.3351 - accuracy: 0.1140 - val_loss: 295.3080 - val_accuracy: 0.0950\n",
      "0.040517032 0.03912923 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 375us/sample - loss: 297.3090 - accuracy: 0.0960 - val_loss: 296.5395 - val_accuracy: 0.1000\n",
      "0.0460192 0.03500725 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 367us/sample - loss: 297.2911 - accuracy: 0.1110 - val_loss: 296.8913 - val_accuracy: 0.1500\n",
      "0.04193545 0.036529887 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 273us/sample - loss: 297.4123 - accuracy: 0.0970 - val_loss: 295.2970 - val_accuracy: 0.1130\n",
      "0.040479556 0.036173668 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 338us/sample - loss: 297.3471 - accuracy: 0.1070 - val_loss: 299.4069 - val_accuracy: 0.1010\n",
      "0.045889437 0.02970438 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 335us/sample - loss: 297.2548 - accuracy: 0.1010 - val_loss: 296.9898 - val_accuracy: 0.1000\n",
      "0.043900065 0.0322937 \n"
     ]
    }
   ],
   "source": [
    "NN = Network()\n",
    "Results_4 = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30,no_of_L=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid',kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid', kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax')))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 887us/sample - loss: 353.4657 - accuracy: 0.1020 - val_loss: 306.6879 - val_accuracy: 0.1000\n",
      "0.74478704 3.245842 1.3125323 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 280us/sample - loss: 308.7908 - accuracy: 0.1250 - val_loss: 308.0294 - val_accuracy: 0.1000\n",
      "0.019993806 0.2083835 0.3974088 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 309.0655 - accuracy: 0.1000 - val_loss: 308.0585 - val_accuracy: 0.1000\n",
      "0.00059595966 0.21338323 0.23056285 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 308.9655 - accuracy: 0.1190 - val_loss: 309.2711 - val_accuracy: 0.1000\n",
      "0.00041475435 0.24593295 0.2110347 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 309.1993 - accuracy: 0.0780 - val_loss: 309.1733 - val_accuracy: 0.1000\n",
      "0.00032996305 0.22968183 0.20002078 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 309.1799 - accuracy: 0.0870 - val_loss: 308.7638 - val_accuracy: 0.1000\n",
      "0.00066182384 0.32175627 0.1485639 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 309.1520 - accuracy: 0.0830 - val_loss: 307.7532 - val_accuracy: 0.1000\n",
      "0.000967243 0.24280336 0.12760052 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 282us/sample - loss: 309.0784 - accuracy: 0.1080 - val_loss: 307.8512 - val_accuracy: 0.1000\n",
      "0.00058500865 0.53414994 0.094475426 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 309.1033 - accuracy: 0.0940 - val_loss: 308.4200 - val_accuracy: 0.1000\n",
      "0.0005622546 0.13508973 0.09515986 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 309.1315 - accuracy: 0.1040 - val_loss: 309.0329 - val_accuracy: 0.1000\n",
      "0.0006886917 0.11715661 0.0897042 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 309.1808 - accuracy: 0.1050 - val_loss: 307.3243 - val_accuracy: 0.1000\n",
      "0.0006455844 0.28507388 0.10454304 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 280us/sample - loss: 309.1738 - accuracy: 0.1050 - val_loss: 308.9456 - val_accuracy: 0.1000\n",
      "0.0005189772 0.308597 0.09352097 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 309.0834 - accuracy: 0.1060 - val_loss: 308.5126 - val_accuracy: 0.1000\n",
      "0.0005410385 0.31796196 0.06866875 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 309.1101 - accuracy: 0.1100 - val_loss: 308.9432 - val_accuracy: 0.1000\n",
      "0.00067513663 0.25101355 0.06538712 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 309.2021 - accuracy: 0.0910 - val_loss: 308.3476 - val_accuracy: 0.1000\n",
      "0.00060351164 0.25367013 0.06605409 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 309.2027 - accuracy: 0.0920 - val_loss: 308.9765 - val_accuracy: 0.1000\n",
      "0.00077104964 0.22002432 0.060058452 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 309.1461 - accuracy: 0.0950 - val_loss: 307.7001 - val_accuracy: 0.1000\n",
      "0.0007725932 0.2853973 0.04911081 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 309.1192 - accuracy: 0.1080 - val_loss: 310.7043 - val_accuracy: 0.1000\n",
      "0.0006916849 0.17994522 0.05881115 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 309.1915 - accuracy: 0.1190 - val_loss: 308.8382 - val_accuracy: 0.1000\n",
      "0.0006245217 0.24440227 0.060269553 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 309.0685 - accuracy: 0.0950 - val_loss: 308.0887 - val_accuracy: 0.1000\n",
      "0.00044633084 0.45344618 0.041225325 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 309.1347 - accuracy: 0.1100 - val_loss: 308.7271 - val_accuracy: 0.1000\n",
      "0.00053875503 0.18712379 0.04560619 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 309.1214 - accuracy: 0.0970 - val_loss: 309.0289 - val_accuracy: 0.1000\n",
      "0.0006351528 0.1326133 0.0420811 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 345us/sample - loss: 309.1698 - accuracy: 0.0920 - val_loss: 308.6345 - val_accuracy: 0.1000\n",
      "0.000533761 0.3036659 0.04484735 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 309.1249 - accuracy: 0.1050 - val_loss: 310.3634 - val_accuracy: 0.1000\n",
      "0.00064984773 0.38712153 0.0465035 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 309.1622 - accuracy: 0.0990 - val_loss: 309.1278 - val_accuracy: 0.1000\n",
      "0.0005551444 0.23232317 0.042063214 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 309.1409 - accuracy: 0.1010 - val_loss: 309.0259 - val_accuracy: 0.1000\n",
      "0.0005409006 0.30013278 0.04547344 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 309.1215 - accuracy: 0.1110 - val_loss: 308.0915 - val_accuracy: 0.1000\n",
      "0.00060892414 0.22943652 0.03512844 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 278us/sample - loss: 309.2003 - accuracy: 0.0800 - val_loss: 310.5441 - val_accuracy: 0.1000\n",
      "0.00055728515 0.22338647 0.03387078 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 288us/sample - loss: 309.1461 - accuracy: 0.1010 - val_loss: 311.9760 - val_accuracy: 0.1000\n",
      "0.00075282546 0.4558411 0.031167183 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 309.1878 - accuracy: 0.0990 - val_loss: 309.3024 - val_accuracy: 0.1000\n",
      "0.0005105592 0.41065347 0.03180791 \n"
     ]
    }
   ],
   "source": [
    "NN = Network()\n",
    "Results_5 = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30,no_of_L = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add((tf.keras.layers.Dense(30, input_shape = (784,), activation = 'sigmoid',kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid', kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(30, activation = 'sigmoid',kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.add((tf.keras.layers.Dense(10, activation = 'softmax',kernel_regularizer = tf.keras.regularizers.l2(5), bias_regularizer=tf.keras.regularizers.l2(5))))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr = 0.1), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 367.3213 - accuracy: 0.1120 - val_loss: 322.9376 - val_accuracy: 0.1000\n",
      "0.479771 2.602832 2.7426746 6.528477 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 377us/sample - loss: 323.9634 - accuracy: 0.1030 - val_loss: 323.5657 - val_accuracy: 0.1000\n",
      "0.023636675 0.013264712 0.0136688575 1.6356825 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 323.9793 - accuracy: 0.0930 - val_loss: 323.2752 - val_accuracy: 0.1000\n",
      "0.00016801158 0.0048898137 0.0062163994 0.804884 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 324.0222 - accuracy: 0.1030 - val_loss: 326.6356 - val_accuracy: 0.1000\n",
      "0.00010647993 0.0050560622 0.010200452 1.3031311 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 323.9632 - accuracy: 0.0970 - val_loss: 323.5609 - val_accuracy: 0.1000\n",
      "9.374946e-05 0.004274606 0.026776142 2.9311507 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 323.9986 - accuracy: 0.0950 - val_loss: 322.1465 - val_accuracy: 0.1000\n",
      "8.696934e-05 0.0035868827 0.0040434394 2.3115141 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 289us/sample - loss: 323.9290 - accuracy: 0.1070 - val_loss: 324.9016 - val_accuracy: 0.1000\n",
      "0.00010150375 0.010175677 0.027069794 1.1029948 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 324.0624 - accuracy: 0.0980 - val_loss: 323.5056 - val_accuracy: 0.1000\n",
      "0.00014138123 0.009618214 0.03057923 2.4199011 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 323.9123 - accuracy: 0.0970 - val_loss: 323.7994 - val_accuracy: 0.1000\n",
      "0.000107077736 0.006882462 0.010183451 3.4452107 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 324.0050 - accuracy: 0.1040 - val_loss: 324.2989 - val_accuracy: 0.1000\n",
      "7.978948e-05 0.0009881405 0.008955387 0.99822164 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 323.9916 - accuracy: 0.0890 - val_loss: 324.5718 - val_accuracy: 0.1000\n",
      "8.366224e-05 0.004679442 0.014676654 1.3459752 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 402us/sample - loss: 323.9697 - accuracy: 0.1150 - val_loss: 324.3989 - val_accuracy: 0.1000\n",
      "7.3820265e-05 0.0019791755 0.009431519 0.9347752 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 348us/sample - loss: 324.0213 - accuracy: 0.0930 - val_loss: 324.3151 - val_accuracy: 0.1000\n",
      "8.6194734e-05 0.0044232537 0.02511507 3.5767725 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 340us/sample - loss: 323.9783 - accuracy: 0.0930 - val_loss: 325.1400 - val_accuracy: 0.1000\n",
      "0.00011362194 0.0055958307 0.0076569114 1.2927324 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 374us/sample - loss: 323.9791 - accuracy: 0.1070 - val_loss: 323.3512 - val_accuracy: 0.1000\n",
      "9.9878e-05 0.0028725904 0.007154151 1.4926724 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 323.9969 - accuracy: 0.1010 - val_loss: 324.0741 - val_accuracy: 0.1000\n",
      "9.579361e-05 0.0014419861 0.016089821 1.1539624 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 335us/sample - loss: 323.9882 - accuracy: 0.1100 - val_loss: 323.5415 - val_accuracy: 0.1000\n",
      "9.859995e-05 0.0033211843 0.021154968 1.6128217 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 323.9054 - accuracy: 0.1010 - val_loss: 323.1866 - val_accuracy: 0.1000\n",
      "7.680171e-05 0.0007556445 0.005964943 1.8006803 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 324.0127 - accuracy: 0.1090 - val_loss: 325.2483 - val_accuracy: 0.1000\n",
      "0.0001226327 0.007681979 0.03190916 1.313493 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 324.0689 - accuracy: 0.0870 - val_loss: 325.1761 - val_accuracy: 0.1000\n",
      "9.229503e-05 0.001880264 0.003069601 1.0455296 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 309us/sample - loss: 323.9604 - accuracy: 0.1040 - val_loss: 323.0097 - val_accuracy: 0.1000\n",
      "0.000108275235 0.0034923588 0.026101721 1.299065 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 326us/sample - loss: 324.0513 - accuracy: 0.1090 - val_loss: 325.5888 - val_accuracy: 0.1000\n",
      "0.00012361932 0.0021683367 0.043625444 0.8136531 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 323.9721 - accuracy: 0.1040 - val_loss: 322.8699 - val_accuracy: 0.1000\n",
      "9.096674e-05 0.007011602 0.0072216215 1.3674704 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 323.9710 - accuracy: 0.1020 - val_loss: 327.6906 - val_accuracy: 0.1000\n",
      "7.198727e-05 0.008010628 0.021859173 0.72525615 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 336us/sample - loss: 324.1287 - accuracy: 0.0950 - val_loss: 324.2413 - val_accuracy: 0.1000\n",
      "7.981635e-05 0.0076266844 0.011316208 1.2344581 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 360us/sample - loss: 323.8796 - accuracy: 0.0900 - val_loss: 323.1639 - val_accuracy: 0.1000\n",
      "8.528421e-05 0.007891334 0.010685888 2.043542 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 456us/sample - loss: 324.0601 - accuracy: 0.1100 - val_loss: 325.1762 - val_accuracy: 0.1000\n",
      "9.217307e-05 0.0051860334 0.013383357 1.234644 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 351us/sample - loss: 324.0110 - accuracy: 0.1000 - val_loss: 323.3823 - val_accuracy: 0.1000\n",
      "8.259068e-05 0.0015059741 0.011544671 1.5296221 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 377us/sample - loss: 324.0380 - accuracy: 0.0910 - val_loss: 323.5207 - val_accuracy: 0.1000\n",
      "7.14547e-05 0.0030098439 0.00566205 1.1661197 \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 323.9779 - accuracy: 0.1020 - val_loss: 323.3400 - val_accuracy: 0.1000\n",
      "8.123887e-05 0.0004963075 0.009385832 3.2461035 \n"
     ]
    }
   ],
   "source": [
    "NN = Network()\n",
    "Results_6 = train(NN,x_train_1000,y_train_1000,x_test_1000,y_test_1000,30,no_of_L=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x200ba6d30f0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_train_1000[5].reshape(28,28)\n",
    "plt.imshow(img, cmap = \"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rotate_img(img, angle):\n",
    "    img_center = tuple(np.array(img.shape[1::-1])/2)\n",
    "    rot_matrix = cv2.getRotationMatrix2D(img_center, angle, 1.0)\n",
    "    res = cv2.warpAffine(img, rot_matrix, img.shape[1::-1], flags = cv2.INTER_LINEAR)\n",
    "    return res\n",
    "\n",
    "plt.imshow(rotate_img(img, 45), cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = np.array([[1,0,10],[0,1,10]])\n",
    "def shift_img(img, tx, ty):\n",
    "    M = np.float32([[1,0,tx],[0,1,ty]])\n",
    "    res = cv2.warpAffine(img, M, img.shape[1::-1])\n",
    "    return res\n",
    "plt.imshow(shift_img(img,3,3),cmap='Reds');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = [-3,0,3]\n",
    "d = [360-3, 360-2, 360-1, 1, 2, 3]\n",
    "\n",
    "X_train_new = x_train_1000.copy()\n",
    "X_test_new = x_test_1000.copy()\n",
    "r = random.randint(0,1000)\n",
    "\n",
    "for i in range(X_train_new.shape[0]):\n",
    "    img = X_train_new[i].reshape(28,28)\n",
    "    angle = random.sample(d,1)[0]\n",
    "    tx,ty = random.sample(t, 2)\n",
    "    \n",
    "    img = rotate_img(img, angle)\n",
    "    img = shift_img(img, tx, ty)\n",
    "    if(i == r):\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.imshow(X_train_new[r].reshape(28,28), cmap = 'Reds')\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.imshow(img, cmap = 'Reds')\n",
    "\n",
    "    X_train_new[i] = img.reshape(X_train_new[i].shape)\n",
    "\n",
    "r = random.randint(0,1000)\n",
    "\n",
    "for i in range(X_test_new.shape[0]):\n",
    "    img = X_test_new[i].reshape(28,28)\n",
    "    angle = random.sample(d,1)[0]\n",
    "    tx,ty = random.sample(t, 2)\n",
    "\n",
    "    img = rotate_img(img, angle)\n",
    "    img = shift_img(img, tx, ty)\n",
    "    if(i == r):\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.imshow(X_test_new[r].reshape(28,28), cmap = 'Reds')\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.imshow(img, cmap = 'Reds')\n",
    "\n",
    "    X_test_new[i] = img.reshape(X_test_new[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), input_shape = (28,28,1), activation = 'relu',padding = 'same'))\n",
    "    model.add(tf.keras.layers.Dropout(0.20))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.20))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.20))\n",
    "    model.add(tf.keras.layers.Dense(10,activation = 'softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr = 0.01), metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(Network,x_train_1000,y_train_1000,x_test_1000,y_test_1000,epochs):\n",
    "    Network_Acc = []\n",
    "    Network_VAcc = []\n",
    "    Network_Train_Loss = []\n",
    "    Network_Train_VLoss = []\n",
    "    Network_ConvL1_Chagne, Network_ConvL2_Chagne, Network_DenseL1_Chagne, Network_DenseL2_Chagne = [], [], [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        ConvL1_Prev = np.array(Network.get_layer(index = 0).get_weights()[0])\n",
    "        ConvL2_Prev = np.array(Network.get_layer(index = 3).get_weights()[0])\n",
    "        DenseL1_Prev = np.array(Network.get_layer(index = 7).get_weights()[0])\n",
    "        DenseL2_Prev = np.array(Network.get_layer(index = 9).get_weights()[0]) \n",
    "\n",
    "        History_1 = Network.fit(x = x_train_1000, y= y_train_1000, batch_size= 10, epochs=1, validation_data=(x_test_1000, y_test_1000))\n",
    "        Network_Acc.append(History_1.history['accuracy'])\n",
    "        Network_VAcc.append(History_1.history['val_accuracy'])\n",
    "        Network_Train_Loss.append(History_1.history['loss'])\n",
    "        Network_Train_VLoss.append(History_1.history['val_loss'])\n",
    "\n",
    "\n",
    "        ConvL1_new = np.array(Network.get_layer(index = 0).get_weights()[0])\n",
    "        ConvL2_new = np.array(Network.get_layer(index = 3).get_weights()[0])\n",
    "        DenseL1_new = np.array(Network.get_layer(index = 7).get_weights()[0])\n",
    "        DenseL2_new = np.array(Network.get_layer(index = 9).get_weights()[0])\n",
    "\n",
    "        Avg_ConvL1_W_Change = np.average(np.absolute((ConvL1_Prev - ConvL1_new)/ConvL1_new))\n",
    "        Avg_ConvL2_W_Change = np.average(np.absolute((ConvL2_Prev - ConvL2_new)/ConvL2_new))\n",
    "        Avg_DenseL1_W_Change = np.average(np.absolute((DenseL1_Prev - DenseL1_new)/DenseL1_new))\n",
    "        Avg_DenseL2_W_Change = np.average(np.absolute((DenseL2_Prev - DenseL2_new)/DenseL2_new))\n",
    "\n",
    "        Network_ConvL1_Chagne.append(Avg_ConvL1_W_Change)\n",
    "        Network_ConvL2_Chagne.append(Avg_ConvL2_W_Change)\n",
    "        Network_DenseL1_Chagne.append(Avg_DenseL1_W_Change)\n",
    "        Network_DenseL2_Chagne.append(Avg_DenseL2_W_Change)\n",
    "\n",
    "        print(Avg_ConvL1_W_Change, Avg_ConvL2_W_Change, Avg_DenseL1_W_Change, Avg_DenseL1_W_Change)\n",
    "\n",
    "    return [Network_Acc, Network_VAcc, Network_Train_Loss, Network_Train_VLoss, Network_ConvL1_Chagne, Network_ConvL2_Chagne, Network_DenseL1_Chagne, Network_DenseL2_Chagne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.8323 - accuracy: 0.7800 - val_loss: 0.3860 - val_accuracy: 0.8810\n",
      "0.18444632 0.111757 0.23706675 0.23706675\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.5674 - accuracy: 0.8290 - val_loss: 0.4341 - val_accuracy: 0.8610\n",
      "0.2720692 0.1521808 5.5466385 5.5466385\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.4549 - accuracy: 0.8640 - val_loss: 0.4298 - val_accuracy: 0.8770\n",
      "0.11470037 0.07955724 0.21437964 0.21437964\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.3835 - accuracy: 0.8770 - val_loss: 0.3650 - val_accuracy: 0.8940\n",
      "0.18104027 0.2504439 0.32936966 0.32936966\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.3498 - accuracy: 0.9010 - val_loss: 0.3514 - val_accuracy: 0.8910\n",
      "0.28511354 0.21941358 0.14652748 0.14652748\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.3026 - accuracy: 0.8990 - val_loss: 0.3598 - val_accuracy: 0.8950\n",
      "0.11403055 0.07859264 0.9307107 0.9307107\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2057 - accuracy: 0.9240 - val_loss: 0.3697 - val_accuracy: 0.8960\n",
      "0.2196557 0.06842736 0.15819353 0.15819353\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2199 - accuracy: 0.9270 - val_loss: 0.3683 - val_accuracy: 0.9000\n",
      "0.12970151 0.14652576 0.15589139 0.15589139\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1983 - accuracy: 0.9400 - val_loss: 0.3276 - val_accuracy: 0.8980\n",
      "0.080116846 0.05520526 0.29376748 0.29376748\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2233 - accuracy: 0.9290 - val_loss: 0.3910 - val_accuracy: 0.8820\n",
      "0.1758496 0.11611536 0.27590978 0.27590978\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2006 - accuracy: 0.9270 - val_loss: 0.4003 - val_accuracy: 0.8820\n",
      "0.103657834 0.09245254 0.2504191 0.2504191\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1899 - accuracy: 0.9420 - val_loss: 0.3367 - val_accuracy: 0.9000\n",
      "0.07971295 0.0664719 0.24418674 0.24418674\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2148 - accuracy: 0.9290 - val_loss: 0.3741 - val_accuracy: 0.8940\n",
      "0.18326943 0.41056335 0.14096896 0.14096896\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2125 - accuracy: 0.9370 - val_loss: 0.4061 - val_accuracy: 0.8910\n",
      "0.12896536 0.1548434 0.30776596 0.30776596\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1795 - accuracy: 0.9480 - val_loss: 0.4110 - val_accuracy: 0.8910\n",
      "0.36696437 0.11985156 0.15169941 0.15169941\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1904 - accuracy: 0.9470 - val_loss: 0.4093 - val_accuracy: 0.8790\n",
      "0.062744506 0.449525 0.28197914 0.28197914\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2009 - accuracy: 0.9320 - val_loss: 0.4796 - val_accuracy: 0.8780\n",
      "0.10579613 0.054094043 0.17468095 0.17468095\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1370 - accuracy: 0.9590 - val_loss: 0.4056 - val_accuracy: 0.8840\n",
      "0.09169607 0.089474864 0.14115202 0.14115202\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.3006 - accuracy: 0.9160 - val_loss: 0.5198 - val_accuracy: 0.8800\n",
      "0.077034615 0.11199485 0.61056334 0.61056334\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2600 - accuracy: 0.9320 - val_loss: 0.4599 - val_accuracy: 0.8940\n",
      "0.098982215 0.08226614 0.23616059 0.23616059\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2095 - accuracy: 0.9300 - val_loss: 0.4567 - val_accuracy: 0.8820\n",
      "0.17047247 0.1351963 0.15171033 0.15171033\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1956 - accuracy: 0.9380 - val_loss: 0.5146 - val_accuracy: 0.8830\n",
      "0.088777095 0.09971556 0.15491362 0.15491362\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1857 - accuracy: 0.9530 - val_loss: 0.4005 - val_accuracy: 0.8850\n",
      "0.11317139 0.060654122 0.15812448 0.15812448\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1922 - accuracy: 0.9430 - val_loss: 0.4152 - val_accuracy: 0.8810\n",
      "0.10602222 0.04900197 0.14518197 0.14518197\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2262 - accuracy: 0.9450 - val_loss: 0.5402 - val_accuracy: 0.8610\n",
      "0.12819539 0.098439425 0.14789517 0.14789517\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2139 - accuracy: 0.9320 - val_loss: 0.4394 - val_accuracy: 0.8750\n",
      "1.511161 0.10554394 0.14927116 0.14927116\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2548 - accuracy: 0.9260 - val_loss: 0.5025 - val_accuracy: 0.8770\n",
      "0.10384419 0.2000254 0.29596516 0.29596516\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1833 - accuracy: 0.9460 - val_loss: 0.4784 - val_accuracy: 0.8770\n",
      "0.07152763 0.07469792 0.32292762 0.32292762\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2000 - accuracy: 0.9450 - val_loss: 0.4749 - val_accuracy: 0.8780\n",
      "0.059562273 0.09717175 0.13211745 0.13211745\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1172 - accuracy: 0.9650 - val_loss: 0.5122 - val_accuracy: 0.8830\n",
      "0.18749261 0.10953666 0.10408444 0.10408444\n"
     ]
    }
   ],
   "source": [
    "CNN = CNN_model()\n",
    "\n",
    "X_train_new = X_train_new.reshape(X_train_new.shape[0], 28,28,1)\n",
    "X_test_new = X_test_new.reshape(X_test_new.shape[0], 28,28,1)\n",
    "\n",
    "Results_7 = train_CNN(N7, X_train_new, y_train_1000, X_test_new, y_test_1000, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
